{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies and custom modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Clear the Python kernel's memory cache\n",
    "# import gc\n",
    "# gc.collect()\n",
    "\n",
    "# Import all required dependencies\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pvlib\n",
    "from shapely.geometry import Polygon\n",
    "import fiona\n",
    "import folium\n",
    "from folium import Choropleth\n",
    "import branca.colormap as cm\n",
    "import json\n",
    "import requests\n",
    "from folium.plugins import Fullscreen, MeasureControl\n",
    "from pvlib.location import Location\n",
    "import warnings\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from shapely.ops import transform\n",
    "import pyproj\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available layers in geodatabase: ['NYC_POIS', 'NYC_NSI', 'NYC_Buildings', 'NYC_BBox', 'NYC_POFW', 'FDNY_Firehouses']\n",
      "\n",
      "Dataset Summary:\n",
      "Number of buildings: 1,084,413\n",
      "Number of Places of Interest: 25,292\n",
      "Number of Places of Worship: 1,106\n",
      "\n",
      "Buildings sample columns:\n",
      "['base_bbl', 'bin', 'cnstrct_yr', 'doitt_id', 'feat_code', 'geomsource', 'groundelev', 'heightroof', 'date_lstmo', 'time_lstmo', 'lststatype', 'mpluto_bbl', 'name', 'Shape_Length', 'Shape_Area', 'geometry']\n",
      "\n",
      "POIs sample columns:\n",
      "['osm_id', 'code', 'fclass', 'name', 'geometry']\n"
     ]
    }
   ],
   "source": [
    "# Second cell - Set paths and load data\n",
    "# Set base input directory\n",
    "INPUT_DIR = '/Users/oliveratwood/One Architecture Dropbox/Oliver Atwood/P2415_CSC Year Two/05 GIS/06 Scripts/ResilienceHub/Input'\n",
    "\n",
    "# Derive paths for input files\n",
    "gdb_path = f\"{INPUT_DIR}/ResilienceHub.gdb\"\n",
    "\n",
    "def load_gdb_layers():\n",
    "    # List all layers in the geodatabase\n",
    "    layers = fiona.listlayers(gdb_path)\n",
    "    print(f\"Available layers in geodatabase: {layers}\")\n",
    "\n",
    "    # Load required layers\n",
    "    buildings = gpd.read_file(gdb_path, layer='NYC_Buildings')\n",
    "    pois = gpd.read_file(gdb_path, layer='NYC_POIS')\n",
    "    pofw = gpd.read_file(gdb_path, layer='NYC_POFW')\n",
    "    # nsi = gpd.read_file(gdb_path, layer='NYC_NSI')\n",
    "\n",
    "    return buildings, pois, pofw\n",
    "\n",
    "# Load the data\n",
    "# buildings, pois, pofw, nsi = load_gdb_layers()\n",
    "buildings, pois, pofw = load_gdb_layers()\n",
    "\n",
    "print(\"\\nDataset Summary:\")\n",
    "print(f\"Number of buildings: {len(buildings):,}\")\n",
    "print(f\"Number of Places of Interest: {len(pois):,}\")\n",
    "print(f\"Number of Places of Worship: {len(pofw):,}\")\n",
    "\n",
    "# Display sample of each dataset\n",
    "print(\"\\nBuildings sample columns:\")\n",
    "print(buildings.columns.tolist())\n",
    "print(\"\\nPOIs sample columns:\")\n",
    "print(pois.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set paths and load data\n",
    "# import geopandas as gpd\n",
    "# import fiona\n",
    "# import gc\n",
    "\n",
    "# # Set base input directory\n",
    "# INPUT_DIR = '/Users/oliveratwood/One Architecture Dropbox/Oliver Atwood/P2415_CSC Year Two/05 GIS/06 Scripts/ResilienceHub/Input'\n",
    "\n",
    "# # Derive paths for input files\n",
    "# gdb_path = f\"{INPUT_DIR}/ResilienceHub.gdb\"\n",
    "\n",
    "# # Clear the Python kernel's memory cache\n",
    "# gc.collect()\n",
    "\n",
    "# def load_gdb_layers():\n",
    "#     try:\n",
    "#         # List all layers in the geodatabase\n",
    "#         layers = fiona.listlayers(gdb_path)\n",
    "#         print(f\"Available layers in geodatabase: {layers}\")\n",
    "\n",
    "#         # Load required layers\n",
    "#         buildings = gpd.read_file(gdb_path, layer='NYC_Buildings')\n",
    "#         pois = gpd.read_file(gdb_path, layer='NYC_POIS')\n",
    "#         pofw = gpd.read_file(gdb_path, layer='NYC_POFW')\n",
    "#         firehouses = gpd.read_file(gdb_path, layer='FDNY_Firehouses', driver='OpenFileGDB')\n",
    "\n",
    "#         # Verify the data load\n",
    "#         print(\"\\nVerifying data load:\")\n",
    "#         for name, data in [('buildings', buildings), ('pois', pois), \n",
    "#                           ('pofw', pofw), ('firehouses', firehouses)]:\n",
    "#             print(f\"\\n{name.upper()} columns:\")\n",
    "#             print(data.columns.tolist())\n",
    "#             print(f\"Number of records: {len(data)}\")\n",
    "\n",
    "#         return buildings, pois, pofw, firehouses\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading data: {str(e)}\")\n",
    "#         raise\n",
    "\n",
    "# # Load the data\n",
    "# buildings, pois, pofw, firehouses = load_gdb_layers()\n",
    "\n",
    "# # Print summary statistics\n",
    "# print(\"\\nDataset Summary:\")\n",
    "# print(f\"Number of buildings: {len(buildings):,}\")\n",
    "# print(f\"Number of Places of Interest: {len(pois):,}\")\n",
    "# print(f\"Number of Places of Worship: {len(pofw):,}\")\n",
    "# print(f\"Number of Firehouses: {len(firehouses):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Diagnostic code to examine firehouses dataset\n",
    "# print(\"\\nFirehouses Dataset Information:\")\n",
    "# print(\"Columns:\")\n",
    "# print(firehouses.columns.tolist())\n",
    "# print(\"\\nShape:\", firehouses.shape)\n",
    "# print(\"\\nData Types:\")\n",
    "# print(firehouses.dtypes)\n",
    "\n",
    "# # Try to display all columns, not just geometry\n",
    "# print(\"\\nFirst few rows with all columns:\")\n",
    "# print(firehouses.head().to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter and prepare candidate buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered POIs by class:\n",
      "fclass\n",
      "school              718\n",
      "kindergarten        115\n",
      "arts_centre          83\n",
      "community_centre     68\n",
      "library              56\n",
      "college              22\n",
      "shelter               7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Merged candidates by class:\n",
      "fclass\n",
      "pofw_christian                731\n",
      "school                        718\n",
      "kindergarten                  115\n",
      "pofw_jewish                   112\n",
      "arts_centre                    83\n",
      "pofw_christian_catholic        71\n",
      "community_centre               68\n",
      "pofw_christian_lutheran        59\n",
      "library                        56\n",
      "pofw_christian_methodist       50\n",
      "pofw_muslim                    35\n",
      "college                        22\n",
      "pofw_buddhist                  18\n",
      "pofw_hindu                      9\n",
      "pofw_muslim_sunni               9\n",
      "shelter                         7\n",
      "pofw_christian_orthodox         4\n",
      "pofw_christian_protestant       3\n",
      "pofw_christian_anglican         2\n",
      "pofw_sikh                       2\n",
      "pofw_christian_evangelical      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final candidate buildings:\n",
      "Total buildings selected: 1,776\n",
      "\n",
      "Distribution by facility type:\n",
      "fclass\n",
      "school                        718\n",
      "pofw_christian                465\n",
      "kindergarten                  115\n",
      "arts_centre                    83\n",
      "community_centre               68\n",
      "pofw_jewish                    60\n",
      "library                        56\n",
      "pofw_christian_catholic        42\n",
      "pofw_christian_lutheran        38\n",
      "pofw_muslim                    31\n",
      "pofw_christian_methodist       28\n",
      "college                        22\n",
      "pofw_buddhist                  16\n",
      "pofw_hindu                      9\n",
      "pofw_muslim_sunni               9\n",
      "shelter                         7\n",
      "pofw_christian_orthodox         3\n",
      "pofw_sikh                       2\n",
      "pofw_christian_protestant       2\n",
      "pofw_christian_evangelical      1\n",
      "pofw_christian_anglican         1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filter and prepare candidate buildings\n",
    "def filter_pois(pois):\n",
    "    target_classes = [\n",
    "        'arts_centre', 'college', 'community_centre', \n",
    "        'kindergarten', 'library', 'school', 'shelter'\n",
    "    ]\n",
    "    filtered_pois = pois[pois['fclass'].isin(target_classes)].copy()\n",
    "    print(f\"\\nFiltered POIs by class:\")\n",
    "    print(filtered_pois['fclass'].value_counts())\n",
    "    return filtered_pois\n",
    "\n",
    "def merge_candidates(filtered_pois, pofw):\n",
    "    # Prepare POFW by adding prefix to fclass\n",
    "    pofw_prep = pofw.copy()\n",
    "    pofw_prep['fclass'] = 'pofw_' + pofw_prep['fclass']\n",
    "\n",
    "    # Select columns to keep\n",
    "    cols_to_keep = ['fclass', 'name', 'geometry']\n",
    "\n",
    "    # Merge datasets\n",
    "    candidates = pd.concat([\n",
    "        filtered_pois[cols_to_keep],\n",
    "        pofw_prep[cols_to_keep]\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    # Add ObjectID\n",
    "    candidates['ObjectID'] = candidates.index + 1\n",
    "\n",
    "    print(f\"\\nMerged candidates by class:\")\n",
    "    print(candidates['fclass'].value_counts())\n",
    "    return candidates\n",
    "\n",
    "# Process candidates\n",
    "filtered_pois = filter_pois(pois)\n",
    "candidate_points = merge_candidates(filtered_pois, pofw)\n",
    "\n",
    "# Select building footprints\n",
    "candidate_buildings = gpd.sjoin(\n",
    "    buildings,\n",
    "    candidate_points,\n",
    "    how='inner',\n",
    "    predicate='contains'\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal candidate buildings:\")\n",
    "print(f\"Total buildings selected: {len(candidate_buildings):,}\")\n",
    "print(\"\\nDistribution by facility type:\")\n",
    "print(candidate_buildings['fclass'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter and prepare candidate buildings\n",
    "# def filter_pois(pois):\n",
    "#     target_classes = [\n",
    "#         'arts_centre', 'college', 'community_centre', \n",
    "#         'kindergarten', 'library', 'school', 'shelter'\n",
    "#     ]\n",
    "#     filtered_pois = pois[pois['fclass'].isin(target_classes)].copy()\n",
    "#     print(f\"\\nFiltered POIs by class:\")\n",
    "#     print(filtered_pois['fclass'].value_counts())\n",
    "#     return filtered_pois\n",
    "\n",
    "# def prepare_firehouses(firehouses):\n",
    "#     # Create a copy of the firehouses dataset\n",
    "#     firehouses_prep = firehouses.copy()\n",
    "\n",
    "#     # Add required columns with appropriate values\n",
    "#     firehouses_prep['fclass'] = 'Firehouse'\n",
    "#     firehouses_prep['name'] = firehouses_prep['FacilityName']\n",
    "\n",
    "#     # Select only needed columns\n",
    "#     cols_to_keep = ['fclass', 'name', 'geometry']\n",
    "#     firehouses_prep = firehouses_prep[cols_to_keep]\n",
    "\n",
    "#     print(f\"\\nPrepared firehouses:\")\n",
    "#     print(f\"Total firehouses: {len(firehouses_prep)}\")\n",
    "#     return firehouses_prep\n",
    "\n",
    "# def merge_candidates(filtered_pois, pofw, firehouses):\n",
    "#     # Prepare POFW by adding prefix to fclass\n",
    "#     pofw_prep = pofw.copy()\n",
    "#     pofw_prep['fclass'] = 'pofw_' + pofw_prep['fclass']\n",
    "\n",
    "#     # Prepare firehouses\n",
    "#     firehouses_prep = prepare_firehouses(firehouses)\n",
    "\n",
    "#     # Select columns to keep\n",
    "#     cols_to_keep = ['fclass', 'name', 'geometry']\n",
    "\n",
    "#     # Merge all three datasets\n",
    "#     candidates = pd.concat([\n",
    "#         filtered_pois[cols_to_keep],\n",
    "#         pofw_prep[cols_to_keep],\n",
    "#         firehouses_prep[cols_to_keep]\n",
    "#     ], ignore_index=True)\n",
    "\n",
    "#     # Add ObjectID\n",
    "#     candidates['ObjectID'] = candidates.index + 1\n",
    "\n",
    "#     print(f\"\\nMerged candidates by class:\")\n",
    "#     print(candidates['fclass'].value_counts())\n",
    "#     return candidates\n",
    "\n",
    "# # Process candidates\n",
    "# filtered_pois = filter_pois(pois)\n",
    "# candidate_points = merge_candidates(filtered_pois, pofw, firehouses)\n",
    "\n",
    "# # Select building footprints\n",
    "# candidate_buildings = gpd.sjoin(\n",
    "#     buildings,\n",
    "#     candidate_points,\n",
    "#     how='inner',\n",
    "#     predicate='contains'\n",
    "# )\n",
    "\n",
    "# print(f\"\\nFinal candidate buildings:\")\n",
    "# print(f\"Total buildings selected: {len(candidate_buildings):,}\")\n",
    "# print(\"\\nDistribution by facility type:\")\n",
    "# print(candidate_buildings['fclass'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Solar Analysis with Import Verification ===\n",
      "Error creating SolarAnalyzer: name 'solar_analysis' is not defined\n",
      "\n",
      "Input Data Verification:\n",
      "Number of candidate buildings: 1776\n",
      "Number of total buildings: 1084413\n",
      "\n",
      "Sample of first building data:\n",
      "- Shape_Area: 3377.7584572166847\n",
      "- Height: 38.0\n",
      "\n",
      "=== Starting Optimized Solar Analysis ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- Height: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_building[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheightroof\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Run analysis with both required arguments\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m analyzed_buildings \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_solar_potential\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate_buildings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuildings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m analyzed_buildings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mERROR: Analysis failed to produce valid results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/One Architecture Dropbox/Oliver Atwood/P2415_CSC Year Two/05 GIS/06 Scripts/ResilienceHub/solar_analysis.py:202\u001b[0m, in \u001b[0;36manalyze_solar_potential\u001b[0;34m(candidate_gdf, full_buildings_gdf)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Optimized parallel solar potential analysis with progress bars\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Starting Optimized Solar Analysis ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 202\u001b[0m start_time \u001b[38;5;241m=\u001b[39m \u001b[43mtime\u001b[49m\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# Optimize memory usage\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOptimizing data structures...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "# Run the solar analysis\n",
    "import time\n",
    "import os\n",
    "from solar_analysis import analyze_solar_potential\n",
    "from run_solar_analysis import run_analysis\n",
    "import multiprocessing as mp\n",
    "\n",
    "print(\"\\n=== Starting Solar Analysis with Import Verification ===\")\n",
    "\n",
    "# Create analyzer instance to verify class access\n",
    "try:\n",
    "    analyzer = solar_analysis.SolarAnalyzer()\n",
    "    print(\"\\nSolarAnalyzer initialization test:\")\n",
    "    print(f\"Monthly radiation data: {analyzer._monthly_radiation}\")\n",
    "    print(f\"Annual radiation calculation: {analyzer._annual_radiation:.2f} kWh/m²/year\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating SolarAnalyzer: {str(e)}\")\n",
    "\n",
    "# First, verify input data\n",
    "print(\"\\nInput Data Verification:\")\n",
    "print(f\"Number of candidate buildings: {len(candidate_buildings)}\")\n",
    "print(f\"Number of total buildings: {len(buildings)}\")\n",
    "print(\"\\nSample of first building data:\")\n",
    "sample_building = candidate_buildings.iloc[0]\n",
    "print(f\"- Shape_Area: {sample_building['Shape_Area']}\")\n",
    "print(f\"- Height: {sample_building['heightroof']}\")\n",
    "\n",
    "# Run analysis with both required arguments\n",
    "analyzed_buildings = analyze_solar_potential(candidate_buildings, buildings)\n",
    "\n",
    "if analyzed_buildings is None:\n",
    "    print(\"\\nERROR: Analysis failed to produce valid results\")\n",
    "    exit()\n",
    "\n",
    "# Enhanced debug prints\n",
    "print(\"\\n=== Detailed Analysis Results ===\")\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(f\"Total buildings processed: {len(analyzed_buildings)}\")\n",
    "print(f\"Columns present: {analyzed_buildings.columns.tolist()}\")\n",
    "\n",
    "# Detailed analysis of first building\n",
    "print(\"\\nDetailed Analysis of First Building:\")\n",
    "first_building = analyzed_buildings.iloc[0]\n",
    "print(\"\\nInput values:\")\n",
    "print(f\"- Area: {first_building['Shape_Area']} sq ft\")\n",
    "print(f\"- Height: {first_building['heightroof']} ft\")\n",
    "print(\"\\nCalculated values:\")\n",
    "print(f\"- Effective area: {first_building['effective_area']:.2f} m²\")\n",
    "print(f\"- Shadow factor: {first_building['shadow_factor']:.2f}\")\n",
    "print(f\"- Annual radiation: {first_building['annual_radiation']:.2f} kWh/m²/year\")\n",
    "print(f\"- Solar potential: {first_building['solar_potential']:.2f} kWh/year\")\n",
    "print(f\"- Peak power: {first_building['peak_power']:.2f} kW\")\n",
    "\n",
    "# Analysis of all buildings\n",
    "print(\"\\n=== Overall Analysis ===\")\n",
    "positive_potential = analyzed_buildings[analyzed_buildings['solar_potential'] > 0.01]\n",
    "print(f\"\\nBuildings with positive solar potential: {len(positive_potential)}\")\n",
    "\n",
    "if len(positive_potential) > 0:\n",
    "    print(\"\\nSolar Potential Statistics:\")\n",
    "    print(f\"Total potential: {positive_potential['solar_potential'].sum():,.0f} kWh/year\")\n",
    "    print(f\"Average potential: {positive_potential['solar_potential'].mean():,.0f} kWh/year\")\n",
    "    print(f\"Maximum potential: {positive_potential['solar_potential'].max():,.0f} kWh/year\")\n",
    "else:\n",
    "    print(\"\\n=== Diagnostic Information ===\")\n",
    "    print(\"\\nKey Metrics:\")\n",
    "    print(f\"Buildings with valid height: {(analyzed_buildings['heightroof'] > 0).sum()}\")\n",
    "    print(f\"Buildings with valid area: {(analyzed_buildings['Shape_Area'] > 0).sum()}\")\n",
    "    print(f\"Buildings with valid shadow factor: {(analyzed_buildings['shadow_factor'] > 0).sum()}\")\n",
    "    print(f\"Buildings with valid annual radiation: {(analyzed_buildings['annual_radiation'] > 0).sum()}\")\n",
    "\n",
    "# Save debug information to file\n",
    "debug_output = Path('results') / f'solar_analysis_debug_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.txt'\n",
    "debug_output.parent.mkdir(exist_ok=True)\n",
    "\n",
    "with open(debug_output, 'w') as f:\n",
    "    f.write(\"=== Solar Analysis Debug Output ===\\n\")\n",
    "    f.write(f\"Total buildings: {len(analyzed_buildings)}\\n\")\n",
    "    f.write(f\"Buildings with positive potential: {len(positive_potential)}\\n\")\n",
    "    f.write(\"\\nColumn Statistics:\\n\")\n",
    "    for col in analyzed_buildings.columns:\n",
    "        if analyzed_buildings[col].dtype in [np.float64, np.int64]:\n",
    "            f.write(f\"\\n{col}:\\n\")\n",
    "            f.write(f\"- Range: {analyzed_buildings[col].min()} to {analyzed_buildings[col].max()}\\n\")\n",
    "            f.write(f\"- Mean: {analyzed_buildings[col].mean()}\\n\")\n",
    "            f.write(f\"- Non-null count: {analyzed_buildings[col].count()}\\n\")\n",
    "\n",
    "print(f\"\\nDebug information saved to: {debug_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'analyzed_buildings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msolar_mapping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_detailed_solar_map\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create and display the map\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m solar_map \u001b[38;5;241m=\u001b[39m create_detailed_solar_map(\u001b[43manalyzed_buildings\u001b[49m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Save the map\u001b[39;00m\n\u001b[1;32m      8\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'analyzed_buildings' is not defined"
     ]
    }
   ],
   "source": [
    "# Import custom module\n",
    "from solar_mapping import create_detailed_solar_map\n",
    "\n",
    "# Create and display the map\n",
    "solar_map = create_detailed_solar_map(analyzed_buildings)\n",
    "\n",
    "# Save the map\n",
    "output_dir = Path('results')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "map_file = output_dir / 'solar_potential_map.html'\n",
    "solar_map.save(str(map_file))\n",
    "\n",
    "# Display the map in the notebook\n",
    "solar_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# heat analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import rasterio\n",
    "# from rasterio.mask import mask\n",
    "# from rasterio.io import MemoryFile\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# def kelvin_to_celsius(k):\n",
    "#     \"\"\"Convert Kelvin to Celsius\"\"\"\n",
    "#     return k - 273.15\n",
    "\n",
    "# def kelvin_to_fahrenheit(k):\n",
    "#     \"\"\"Convert Kelvin to Fahrenheit\"\"\"\n",
    "#     return (k - 273.15) * 9/5 + 32\n",
    "\n",
    "# def create_empty_stats(idx):\n",
    "#     \"\"\"Create empty statistics dictionary\"\"\"\n",
    "#     return {\n",
    "#         'building_id': idx,\n",
    "#         'pixel_count': 0,\n",
    "#         'unique_values': 0,\n",
    "#         'median_temperature_c': np.nan,\n",
    "#         'mean_temperature_c': np.nan,\n",
    "#         'max_temperature_c': np.nan,\n",
    "#         'min_temperature_c': np.nan,\n",
    "#         'temperature_range_c': np.nan,\n",
    "#         'median_temperature_f': np.nan,\n",
    "#         'mean_temperature_f': np.nan,\n",
    "#         'max_temperature_f': np.nan,\n",
    "#         'min_temperature_f': np.nan,\n",
    "#         'temperature_range_f': np.nan,\n",
    "#         'buffer_area_m2': 0\n",
    "#     }\n",
    "\n",
    "# def process_chunk(gdf_chunk, raster_src):\n",
    "#     \"\"\"Process a chunk of buildings using a raster source\"\"\"\n",
    "#     results = []\n",
    "#     debug_count = 0\n",
    "\n",
    "#     # Get the raster's transform and CRS\n",
    "#     transform = raster_src.transform\n",
    "\n",
    "#     for idx, row in gdf_chunk.iterrows():\n",
    "#         try:\n",
    "#             # Create a smaller buffer\n",
    "#             buffer_geom = row.geometry.buffer(30)  # 30 meters buffer\n",
    "\n",
    "#             # Convert to GeoJSON-like dict\n",
    "#             geom = [buffer_geom.__geo_interface__]\n",
    "\n",
    "#             try:\n",
    "#                 # Get window for the geometry\n",
    "#                 building_data, out_transform = mask(raster_src, \n",
    "#                                                  geom, \n",
    "#                                                  crop=True, \n",
    "#                                                  all_touched=False,\n",
    "#                                                  nodata=np.nan)\n",
    "\n",
    "#                 # Flatten and remove NaN values\n",
    "#                 valid_data = building_data.flatten()\n",
    "#                 valid_data = valid_data[~np.isnan(valid_data)]\n",
    "\n",
    "#                 if len(valid_data) > 0:\n",
    "#                     # Debug printing for first few buildings\n",
    "#                     if debug_count < 5:\n",
    "#                         print(f\"\\nBuilding {idx} data summary:\")\n",
    "#                         print(f\"Geometry bounds: {buffer_geom.bounds}\")\n",
    "#                         print(f\"Shape of masked data: {building_data.shape}\")\n",
    "#                         print(f\"Number of valid pixels: {len(valid_data)}\")\n",
    "#                         print(f\"Raw Kelvin values - Min: {valid_data.min():.2f}, Max: {valid_data.max():.2f}\")\n",
    "#                         print(f\"Converted to Celsius - Min: {kelvin_to_celsius(valid_data.min()):.2f}, \"\n",
    "#                               f\"Max: {kelvin_to_celsius(valid_data.max()):.2f}\")\n",
    "#                         print(f\"Unique values: {len(np.unique(valid_data))}\")\n",
    "#                         debug_count += 1\n",
    "\n",
    "#                     # Convert and store statistics\n",
    "#                     valid_data_c = kelvin_to_celsius(valid_data)\n",
    "\n",
    "#                     stats = {\n",
    "#                         'building_id': idx,\n",
    "#                         'pixel_count': len(valid_data),\n",
    "#                         'unique_values': len(np.unique(valid_data)),\n",
    "#                         'median_temperature_c': float(np.median(valid_data_c)),\n",
    "#                         'mean_temperature_c': float(np.mean(valid_data_c)),\n",
    "#                         'max_temperature_c': float(np.max(valid_data_c)),\n",
    "#                         'min_temperature_c': float(np.min(valid_data_c))\n",
    "#                     }\n",
    "\n",
    "#                     # Calculate ranges\n",
    "#                     stats['temperature_range_c'] = stats['max_temperature_c'] - stats['min_temperature_c']\n",
    "\n",
    "#                     # Add Fahrenheit conversions\n",
    "#                     stats.update({\n",
    "#                         'median_temperature_f': (stats['median_temperature_c'] * 9/5) + 32,\n",
    "#                         'mean_temperature_f': (stats['mean_temperature_c'] * 9/5) + 32,\n",
    "#                         'max_temperature_f': (stats['max_temperature_c'] * 9/5) + 32,\n",
    "#                         'min_temperature_f': (stats['min_temperature_c'] * 9/5) + 32,\n",
    "#                         'temperature_range_f': stats['temperature_range_c'] * 9/5\n",
    "#                     })\n",
    "\n",
    "#                 else:\n",
    "#                     raise ValueError(\"No valid data found in masked region\")\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Masking error for building {idx}: {str(e)}\")\n",
    "#                 stats = create_empty_stats(idx)\n",
    "\n",
    "#             # Add buffer area regardless of success\n",
    "#             stats['buffer_area_m2'] = float(buffer_geom.area)\n",
    "#             results.append(stats)\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing building {idx}: {str(e)}\")\n",
    "#             results.append(create_empty_stats(idx))\n",
    "\n",
    "#     return results\n",
    "\n",
    "# def create_detailed_summary(gdf):\n",
    "#     \"\"\"Create detailed summary statistics\"\"\"\n",
    "#     total_sites = len(gdf)\n",
    "#     valid_sites = gdf['mean_temperature_c'].notna().sum()\n",
    "\n",
    "#     summary = {\n",
    "#         'total_sites': total_sites,\n",
    "#         'valid_sites': valid_sites,\n",
    "#         'coverage_percentage': (valid_sites/total_sites)*100 if total_sites > 0 else 0\n",
    "#     }\n",
    "\n",
    "#     if valid_sites > 0:\n",
    "#         # Data coverage statistics\n",
    "#         summary['coverage_stats'] = {\n",
    "#             'total_pixels': int(gdf['pixel_count'].sum()),\n",
    "#             'mean_pixels_per_building': float(gdf['pixel_count'].mean()),\n",
    "#             'median_pixels_per_building': float(gdf['pixel_count'].median()),\n",
    "#             'mean_unique_values': float(gdf['unique_values'].mean())\n",
    "#         }\n",
    "\n",
    "#         # Temperature statistics\n",
    "#         temp_stats = {}\n",
    "#         for unit, cols in [('celsius', '_c'), ('fahrenheit', '_f')]:\n",
    "#             temp_stats[unit] = {\n",
    "#                 'mean': float(gdf[f'mean_temperature{cols}'].mean()),\n",
    "#                 'median': float(gdf[f'median_temperature{cols}'].median()),\n",
    "#                 'max': float(gdf[f'max_temperature{cols}'].max()),\n",
    "#                 'min': float(gdf[f'min_temperature{cols}'].min()),\n",
    "#                 'std': float(gdf[f'mean_temperature{cols}'].std()),\n",
    "#                 'avg_range': float(gdf[f'temperature_range{cols}'].mean())\n",
    "#             }\n",
    "#         summary['temperature_stats'] = temp_stats\n",
    "\n",
    "#         # Area statistics\n",
    "#         summary['area_stats'] = {\n",
    "#             'total_buffer_area': float(gdf['buffer_area_m2'].sum()),\n",
    "#             'mean_buffer_area': float(gdf['buffer_area_m2'].mean()),\n",
    "#             'median_buffer_area': float(gdf['buffer_area_m2'].median())\n",
    "#         }\n",
    "\n",
    "#         # Temperature distribution\n",
    "#         for unit, col in [('celsius', 'mean_temperature_c'), \n",
    "#                          ('fahrenheit', 'mean_temperature_f')]:\n",
    "#             # Calculate quintiles\n",
    "#             quintiles = gdf[col].quantile([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "\n",
    "#             try:\n",
    "#                 temp_bins = pd.cut(gdf[col], \n",
    "#                                  bins=quintiles,\n",
    "#                                  labels=[f'{quintiles[i]:.1f} to {quintiles[i+1]:.1f}' \n",
    "#                                        for i in range(len(quintiles)-1)],\n",
    "#                                  include_lowest=True)\n",
    "\n",
    "#                 temp_dist = temp_bins.value_counts().sort_index()\n",
    "\n",
    "#                 summary[f'temperature_distribution_{unit}'] = {\n",
    "#                     str(bin_label): {\n",
    "#                         'count': int(count),\n",
    "#                         'percentage': float((count/valid_sites)*100)\n",
    "#                     } for bin_label, count in temp_dist.items()\n",
    "#                 }\n",
    "\n",
    "#                 # Add quintile values\n",
    "#                 summary[f'temperature_quintiles_{unit}'] = {\n",
    "#                     f'Q{i}': float(val) for i, val in enumerate(quintiles)\n",
    "#                 }\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"\\nWarning: Distribution calculation failed for {unit}\")\n",
    "#                 print(f\"Data summary for {col}:\")\n",
    "#                 print(gdf[col].describe())\n",
    "#                 print(f\"Error: {str(e)}\")\n",
    "\n",
    "#     return summary\n",
    "\n",
    "# def print_summary(summary):\n",
    "#     \"\"\"Print formatted summary statistics\"\"\"\n",
    "#     print(\"\\n=== Analysis Summary ===\")\n",
    "#     print(f\"Total buildings analyzed: {summary['total_sites']:,}\")\n",
    "#     print(f\"Valid readings: {summary['valid_sites']:,} ({summary['coverage_percentage']:.1f}%)\")\n",
    "\n",
    "#     if summary['valid_sites'] > 0:\n",
    "#         print(\"\\nCoverage Statistics:\")\n",
    "#         cov_stats = summary['coverage_stats']\n",
    "#         print(f\"Total pixels analyzed: {cov_stats['total_pixels']:,}\")\n",
    "#         print(f\"Mean pixels per building: {cov_stats['mean_pixels_per_building']:.1f}\")\n",
    "#         print(f\"Mean unique values per building: {cov_stats['mean_unique_values']:.1f}\")\n",
    "\n",
    "#         print(\"\\nTemperature Statistics:\")\n",
    "\n",
    "#         for unit in ['celsius', 'fahrenheit']:\n",
    "#             symbol = '°C' if unit == 'celsius' else '°F'\n",
    "#             print(f\"\\n{unit.capitalize()}:\")\n",
    "#             temp_stats = summary['temperature_stats'][unit]\n",
    "#             print(f\"Mean temperature: {temp_stats['mean']:.1f}{symbol}\")\n",
    "#             print(f\"Median temperature: {temp_stats['median']:.1f}{symbol}\")\n",
    "#             print(f\"Maximum temperature: {temp_stats['max']:.1f}{symbol}\")\n",
    "#             print(f\"Minimum temperature: {temp_stats['min']:.1f}{symbol}\")\n",
    "#             print(f\"Standard deviation: {temp_stats['std']:.1f}{symbol}\")\n",
    "#             print(f\"Average temperature range: {temp_stats['avg_range']:.1f}{symbol}\")\n",
    "\n",
    "#         print(\"\\nArea Statistics:\")\n",
    "#         area_stats = summary['area_stats']\n",
    "#         print(f\"Total buffer area: {area_stats['total_buffer_area']:,.0f} sq meters\")\n",
    "#         print(f\"Mean buffer area: {area_stats['mean_buffer_area']:,.0f} sq meters\")\n",
    "\n",
    "#         print(\"\\nTemperature Distribution:\")\n",
    "#         for unit in ['celsius', 'fahrenheit']:\n",
    "#             symbol = '°C' if unit == 'celsius' else '°F'\n",
    "#             print(f\"\\n{unit.capitalize()} Quintiles:\")\n",
    "#             quintiles = summary[f'temperature_quintiles_{unit}']\n",
    "#             for q, val in quintiles.items():\n",
    "#                 print(f\"{q}: {val:.1f}{symbol}\")\n",
    "\n",
    "#             print(f\"\\n{unit.capitalize()} Distribution:\")\n",
    "#             for bin_range, data in summary[f'temperature_distribution_{unit}'].items():\n",
    "#                 print(f\"{bin_range}{symbol}: {data['count']:,} buildings ({data['percentage']:.1f}%)\")\n",
    "#     else:\n",
    "#         print(\"\\nNo valid temperature readings found\")\n",
    "\n",
    "# def analyze_heat_exposure(gdf, chunk_size=50):\n",
    "#     \"\"\"Process buildings in chunks using rasterio source\"\"\"\n",
    "#     temp_raster_path = '/Users/oliveratwood/One Architecture Dropbox/Oliver Atwood/P2415_CSC Year Two/05 GIS/02 Data/00 Sources & Packages/Heat/Landsat9_ThermalComposite_ST_B10_2020-2023.tif'\n",
    "\n",
    "#     print(\"Loading raster data...\")\n",
    "#     with rasterio.open(temp_raster_path) as src:\n",
    "#         # Convert GDF to raster CRS first\n",
    "#         gdf = gdf.to_crs(src.crs)\n",
    "\n",
    "#         # Process in chunks\n",
    "#         results = []\n",
    "#         num_chunks = len(gdf) // chunk_size + (1 if len(gdf) % chunk_size else 0)\n",
    "\n",
    "#         print(f\"Processing {len(gdf)} buildings in {num_chunks} chunks...\")\n",
    "#         for i in tqdm(range(0, len(gdf), chunk_size)):\n",
    "#             chunk = gdf.iloc[i:i+chunk_size]\n",
    "#             chunk_results = process_chunk(chunk, src)\n",
    "#             results.extend(chunk_results)\n",
    "\n",
    "#     # Convert results to DataFrame\n",
    "#     results_df = pd.DataFrame(results).set_index('building_id')\n",
    "\n",
    "#     # Merge results with original GDF\n",
    "#     for column in results_df.columns:\n",
    "#         gdf[column] = results_df[column]\n",
    "\n",
    "#     # Create and print detailed summary\n",
    "#     summary = create_detailed_summary(gdf)\n",
    "#     print_summary(summary)\n",
    "\n",
    "#     return gdf, summary\n",
    "\n",
    "# # Usage example:\n",
    "# candidate_buildings, summary = analyze_heat_exposure(candidate_buildings, chunk_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Placeholder functions for additional analyses\n",
    "# def analyze_social_vulnerability(candidate_buildings):\n",
    "#     # Placeholder for social vulnerability analysis\n",
    "#     pass\n",
    "\n",
    "# def analyze_heat_exposure(candidate_buildings):\n",
    "#     # Placeholder for extreme heat exposure analysis\n",
    "#     pass\n",
    "\n",
    "# def analyze_flood_risk(candidate_buildings):\n",
    "#     # Placeholder for flood risk analysis\n",
    "#     pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine all analyses and save final results\n",
    "# def save_final_results(candidate_buildings, solar_results):\n",
    "#     # Combine all analysis results\n",
    "#     final_candidates = candidate_buildings.copy()\n",
    "\n",
    "#     # Add solar analysis results\n",
    "#     # ... [Add other analysis results when implemented] ...\n",
    "\n",
    "#     # Generate filename with current date\n",
    "#     current_date = datetime.now().strftime('%y%m%d')\n",
    "#     output_name = f\"RH_Candidates_{current_date}\"\n",
    "\n",
    "#     # Save to geodatabase (original approach - commented out)\n",
    "#     # final_candidates.to_file(gdb_path, layer=output_name, driver=\"FileGDB\")\n",
    "\n",
    "#     # Save to shapefile instead\n",
    "#     output_shapefile = f\"RH_Candidates_{current_date}.shp\"\n",
    "#     final_candidates.to_file(output_shapefile)\n",
    "\n",
    "#     return final_candidates\n",
    "\n",
    "# final_results = save_final_results(candidate_buildings, solar_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resilience_hubs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
